{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "  langchain \\\n",
    "  langchain-community \\\n",
    "  langchain-text-splitters \\\n",
    "  chromadb \\\n",
    "  sentence-transformers \\\n",
    "  ollama \\\n",
    "  pydantic \\\n",
    "  python-dotenv \\\n",
    "  chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! /Users/sohampatil/.pyenv/versions/3.9.10/bin/python -m pip install --upgrade pip\n",
    "! /Users/sohampatil/.pyenv/versions/3.9.10/bin/python -m pip install langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(\"/Users/sohampatil/Documents/Rewear\")\n",
    "\n",
    "assert PROJECT_ROOT.exists(), \"Project root not found\"\n",
    "print(\"Project root set to:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_repo_documents(root: Path) -> List[Document]:\n",
    "    docs = []\n",
    "    for path in root.rglob(\"*\"):\n",
    "        if any(ex in path.parts for ex in EXCLUDE_DIRS):\n",
    "            continue\n",
    "        if path.suffix in ALLOWED_EXTENSIONS and path.is_file():\n",
    "            try:\n",
    "                content = path.read_text(errors=\"ignore\")\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\"source\": str(path.relative_to(root))}\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"Skipped:\", path, e)\n",
    "    return docs\n",
    "\n",
    "documents = load_repo_documents(PROJECT_ROOT)\n",
    "print(\"Loaded documents:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "chunks=splitter.split_documents(documents)\n",
    "print(\"Created chunks:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade torch sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.schema import Document\n",
    "\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "class HFEmbeddings:\n",
    "    def __init__(self, model_name, api_key):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key\n",
    "        self.api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "        embeddings = []\n",
    "        for text in texts:  # Send one at a time to avoid JSON errors\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                headers=headers,\n",
    "                json={\"inputs\": text, \"options\": {\"wait_for_model\": True}}\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                raise ValueError(f\"HF API error {response.status_code}: {response.text}\")\n",
    "            result = response.json()\n",
    "            if isinstance(result, list):\n",
    "                embeddings.append(result[0])\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected response format: {result}\")\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "class HFEmbeddings:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = os.environ[\"HF_TOKEN\"]\n",
    "        self.api_url = f\"https://router.huggingface.co/hf-inference/models/{model_name}\"\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            self.api_url,\n",
    "            headers=headers,\n",
    "            json={\"inputs\": inputs}\n",
    "        )\n",
    "\n",
    "        if response.status_code == 503:\n",
    "            time.sleep(3)\n",
    "            return self._call(inputs)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"HF error {response.status_code}: {response.text}\")\n",
    "\n",
    "        return response.json()\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        result = self._call(texts)\n",
    "\n",
    "        # Router returns: List[List[float]]\n",
    "        if not isinstance(result, list) or not isinstance(result[0], list):\n",
    "            raise ValueError(f\"Unexpected embedding format: {result}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "texts = [\n",
    "    \"CI/CD pipelines explanation\",\n",
    "    \"Kubernetes deployment strategies\",\n",
    "    \"Monitoring and alerting systems\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts]\n",
    "\n",
    "embeddings = HFEmbeddings(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    emb = embeddings.embed_query(d.page_content)\n",
    "    print(f\"Chunk {i+1} embedding size:\", len(emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "\n",
    "ALLOWED_EXTENSIONS = {\n",
    "    \".tf\", \".yaml\", \".yml\", \".md\", \".sh\", \".json\", \".py\", \".js\"\n",
    "}\n",
    "\n",
    "EXCLUDE_DIRS = {\n",
    "    \".terraform\", \"node_modules\", \".git\", \"build\", \"dist\"\n",
    "}\n",
    "\n",
    "project_dirs = [\n",
    "    \"k8s\", \"modules\", \"rewear-backend\", \"rewear-frontend\", \"terraform-backend\"\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for d in project_dirs:\n",
    "    for root, dirs, files in os.walk(d):\n",
    "        dirs[:] = [x for x in dirs if x not in EXCLUDE_DIRS]\n",
    "\n",
    "        for f in files:\n",
    "            if os.path.splitext(f)[1] in ALLOWED_EXTENSIONS:\n",
    "                path = os.path.join(root, f)\n",
    "                try:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        text = file.read()\n",
    "                        if text.strip():\n",
    "                            docs.append(\n",
    "                                Document(\n",
    "                                    page_content=text,\n",
    "                                    metadata={\"source\": path}\n",
    "                                )\n",
    "                            )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "print(f\"✅ Clean documents loaded: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1️⃣ Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# 2️⃣ Split large documents into smaller chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,   # 1k chars per chunk\n",
    "    chunk_overlap=200  # 100 chars overlap for context\n",
    ")\n",
    "\n",
    "chunked_docs = []\n",
    "for doc in docs:\n",
    "    chunks = splitter.split_text(doc.page_content)\n",
    "    chunked_docs.extend([\n",
    "    Document(page_content=c, metadata=doc.metadata)\n",
    "    for c in chunks\n",
    "])\n",
    "\n",
    "print(f\"✅ Total chunks to embed: {len(chunked_docs)}\")\n",
    "\n",
    "# 3️⃣ Create Chroma vector store\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"rewear-devops\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./vectorstore\"\n",
    ")\n",
    "\n",
    "# 4️⃣ Add documents in **small batches**\n",
    "batch_size = 20\n",
    "for i in range(0, len(chunked_docs), batch_size):\n",
    "    batch = chunked_docs[i:i+batch_size]\n",
    "    vectordb.add_documents(batch)\n",
    "    print(f\"✅ Added batch {i//batch_size + 1} / {(len(chunked_docs)+batch_size-1)//batch_size}\")\n",
    "\n",
    "print(\"✅ Vector store created and ready for similarity search\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Example usage\n",
    "print(\"GROQ_API_KEY loaded:\", bool(GROQ_API_KEY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "print(\" LLM loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, k: int = 6) -> str:\n",
    "    docs = vectordb.similarity_search(question, k=k)\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[{d.metadata.get('source', 'unknown')}]\\n{d.page_content}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a senior DevOps engineer.\n",
    "\n",
    "Use ONLY the repo context below.\n",
    "Be precise. No hallucination.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terraform_agent(question: str):\n",
    "    return rag_query(\n",
    "        f\"\"\"\n",
    "You are a Terraform infrastructure expert.\n",
    "\n",
    "Rules:\n",
    "- Answer ONLY if the question involves Terraform, infrastructure, or IaC\n",
    "- Focus on VPC, EKS, IAM, S3, CloudFront, modules, state, or providers\n",
    "- Do NOT answer Helm, Kubernetes YAML, or application-level questions\n",
    "- Do NOT invent resources or issues\n",
    "- If Terraform code is not provided and required, say:\n",
    "  \"Cannot analyze without Terraform code.\"\n",
    "\n",
    "Task:\n",
    "- Identify concrete Terraform issues or misconfigurations\n",
    "- Avoid generic best practices unless directly relevant\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kubernetes_agent(question: str):\n",
    "    return rag_query(\n",
    "        f\"\"\"\n",
    "You are a Kubernetes expert reviewing configuration files.\n",
    "\n",
    "Rules:\n",
    "- Answer ONLY if the question is Kubernetes-related\n",
    "- Identify whether the issue is in Helm, raw YAML, HPA, Ingress, or Service\n",
    "- If the question refers to a file or path, assume it exists and analyze it\n",
    "- Point out concrete misconfigurations or template bugs\n",
    "- Do NOT give generic advice\n",
    "- If insufficient information, say exactly what is missing\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cicd_agent(question: str):\n",
    "    return rag_query(\n",
    "        f\"\"\"\n",
    "You are a CI/CD and GitOps expert.\n",
    "\n",
    "Rules:\n",
    "- Answer ONLY if the question is related to CI/CD, GitOps, ArgoCD, Helm releases, or GitHub Actions\n",
    "- Do NOT answer Kubernetes manifest debugging unless it directly affects CI/CD or ArgoCD sync\n",
    "- Do NOT speculate or give generic advice\n",
    "- If the question is unrelated to CI/CD, say:\n",
    "  \"This question is not related to CI/CD or GitOps.\"\n",
    "- If file contents are required and not provided, ask for them\n",
    "\n",
    "Task:\n",
    "- Identify real CI/CD or ArgoCD issues\n",
    "- Be specific and concise\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incident_agent(error_log: str):\n",
    "    return rag_query(\n",
    "        f\"\"\"\n",
    "An error occurred in production.\n",
    "\n",
    "Error:\n",
    "{error_log}\n",
    "\n",
    "Analyze root cause using this repo.\n",
    "Suggest safe, step-by-step fix.\n",
    "Do NOT suggest destructive actions.\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(cmd: str):\n",
    "    SAFE_COMMANDS = [\"terraform plan\", \"kubectl get\", \"kubectl describe\", \"aws describe\"]\n",
    "    if not any(cmd.startswith(s) for s in SAFE_COMMANDS):\n",
    "        return \"❌ Command blocked for safety\"\n",
    "\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            cmd, shell=True, stderr=subprocess.STDOUT, text=True\n",
    "        )\n",
    "        return output\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return e.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devops_brain(question: str):\n",
    "    return {\n",
    "        \"terraform\": terraform_agent(question),\n",
    "        \"kubernetes\": kubernetes_agent(question),\n",
    "        \"cicd\": cicd_agent(question),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devops_brain(\n",
    "    \"what is my helm directory doing\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_agent(\n",
    "    \"kubectl cannot connect to server: the server has asked for the client to provide credentials\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
